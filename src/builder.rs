use std::collections::HashSet;
use std::sync::Arc;

use async_lsp::lsp_types as lsp;
use async_lsp::lsp_types::Url as Uri;
use derive_more::Constructor;
use sourcemap::SourceMap;

use crate::parser::{LineCol, Token};
use crate::proxy::PROXY_WORKSPACE;
use crate::state::State;
use crate::types::{
    DocumentIdentifier, DocumentLinkStatement, Source, SourceHash, SourceMapBuilder,
};

pub const BUILD_FILE: &'static str = "build.js.emitted";

#[derive(Debug, Constructor)]
pub struct Build {
    pub content: String,
    pub uri: Uri,

    source_map: SourceMap,
    tokens_count: usize,
}

/// Forwarding
impl Build {
    pub fn sources(&self) -> HashSet<Source> {
        self.source_map
            .sources()
            .map(|s| Source::new(s.into()))
            .collect()
    }

    pub fn forward_src_position(
        &self,
        pos: &lsp::Position,
        pos_source: &Source,
    ) -> Option<lsp::Position> {
        let mut token: Option<sourcemap::Token> = None;

        if !self.sources().contains(pos_source) {
            return None;
        }

        for t in self.source_map.tokens() {
            if t.get_source() != Some(&pos_source) {
                continue;
            }
            if t.get_src_line() == pos.line && t.get_src_col() <= pos.character {
                token = Some(t);
            }
            if t.get_src_line() > pos.line {
                break;
            }
        }

        if let Some(t) = token {
            let line = t.get_dst_line();
            let character = t.get_dst_col() + (pos.character - t.get_src_col());
            Some(lsp::Position::new(line, character))
        } else {
            None
        }
    }

    pub fn forward_src_range(
        &self,
        range: &lsp::Range,
        range_source: &Source,
    ) -> Option<lsp::Range> {
        let build_start_pos = self.forward_src_position(&range.start, range_source);
        let build_end_pos = self.forward_src_position(&range.end, range_source);
        match (build_start_pos, build_end_pos) {
            (Some(start), Some(end)) => Some(lsp::Range::new(start, end)),
            _ => None,
        }
    }

    pub fn forward_build_position(&self, pos: &lsp::Position) -> Option<(lsp::Position, Source)> {
        match self.source_map.lookup_token(pos.line, pos.character) {
            Some(t) if t.get_source().is_none() => return None,
            None => return None,
            Some(t) => {
                let line = t.get_src_line();
                let character = t.get_src_col() + (pos.character - t.get_dst_col());
                let source = t.get_source().expect("forward back token must have source");
                Some((
                    lsp::Position::new(line, character),
                    Source::new(source.into()),
                ))
            }
        }
    }

    pub fn forward_build_range(&self, range: &lsp::Range) -> Option<(lsp::Range, Source)> {
        let source_start_pos = self.forward_build_position(&range.start);
        let source_end_pos = self.forward_build_position(&range.end);
        match (source_start_pos, source_end_pos) {
            (Some((start, source)), Some((end, _))) => Some((lsp::Range::new(start, end), source)),
            _ => None,
        }
    }
}

impl Build {
    #[tracing::instrument(skip_all, fields( doc = uri.as_str().split("/").last().unwrap() ))]
    pub fn create(state: &State, uri: &Uri, prev_build: Option<Arc<Self>>) -> Self {
        let (mut initial_buf, sources_cap, tokens_cap) = {
            match prev_build {
                Some(b) => (
                    String::with_capacity(b.content.len()),
                    b.sources().len(),
                    b.tokens_count,
                ),
                None => (String::new(), 0, 0),
            }
        };

        initial_buf.push_str("/** DO NOT EDIT THIS FILE. Generated build for '");
        initial_buf.push_str(uri.as_str().split("/").last().unwrap());
        initial_buf.push_str("' */\n");

        let default_doc = &state.get_default_doc();
        let new_ctx = || {
            let visited = HashSet::<SourceHash>::with_capacity(sources_cap);
            Context::new(state, default_doc, visited)
        };

        let builder = SourceMapBuilder::with_capacity(tokens_cap, sources_cap);
        let mut emit_content_state = Emit::WithDstContent(initial_buf);
        let mut emit_sourcemap_state = Emit::WithSourceMapBuilderAndDstLine(builder, 1);
        let content_task = || Emit::content(&mut emit_content_state, &mut new_ctx(), uri);
        let sourcemap_task = || Emit::sourcemap(&mut emit_sourcemap_state, &mut new_ctx(), uri);

        // TODO: rebuild only sourcemap on dep_hash eq prev
        rayon::join(sourcemap_task, content_task);

        let (tokens_count, source_map) = match emit_sourcemap_state.finish(state) {
            EmitResult::TokensCountAndSourceMap(count, sm) => (count, sm),
            _ => unreachable!(),
        };
        let content = match emit_content_state.finish(state) {
            EmitResult::Content(content) => content,
            _ => unreachable!(),
        };

        #[cfg(debug_assertions)]
        {
            use base64::prelude::{BASE64_STANDARD, Engine as _};

            let mut sm_json = Vec::new();
            let _ = source_map.to_writer(&mut sm_json);
            let sm_base64 = BASE64_STANDARD.encode(&sm_json);
            let build = format!(
                "{}\n//# sourceMappingURL=data:application/json;base64,{}",
                &content, sm_base64
            );
            let _ = std::fs::write(state.get_project().join(BUILD_FILE), build);
        }

        // TODO: change to <project.join(PROXY_WORKSPACE)>/<source_path>/<source_hash.js>
        let ident = state.get_doc(uri).unwrap().source_ident.to_string();
        let emit_path = state
            .get_project()
            .join(PROXY_WORKSPACE)
            .join(format!("{ident}.js"));
        let emit_uri = Uri::from_file_path(emit_path).unwrap();

        Self::new(content, emit_uri, source_map, tokens_count)
    }
}

#[derive(Constructor)]
struct Context<'a> {
    proxy_state: &'a State,
    defult_document: &'a Uri,
    visited_sources: HashSet<SourceHash>,
}

enum Emit {
    WithSourceMapBuilderAndDstLine(SourceMapBuilder, usize),
    WithDstContent(String),
}

enum EmitResult {
    TokensCountAndSourceMap(usize, sourcemap::SourceMap),
    Content(String),
}

impl Emit {
    fn finish(self, state: &State) -> EmitResult {
        match self {
            Emit::WithDstContent(dst_content) => EmitResult::Content(dst_content),
            Emit::WithSourceMapBuilderAndDstLine(b, _) => {
                EmitResult::TokensCountAndSourceMap(b.tokens.len(), b.into_sourcemap(state))
            }
        }
    }
}

/// SourceMap
impl Emit {
    fn add_source(&mut self, source: Arc<str>) -> u32 {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(builder, _) => builder.add_source_with_id(source),
            _ => unreachable!(),
        }
    }

    fn add_token(&mut self, dst_col: usize, src_line: usize, src_col: usize, src_id: u32) {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(builder, dst_line) => {
                builder.tokens.push(sourcemap::RawToken {
                    dst_line: *dst_line as u32,
                    dst_col: dst_col as u32,
                    src_line: src_line as u32,
                    src_col: src_col as u32,
                    src_id,
                    name_id: !0,
                    is_range: false,
                })
            }
            _ => unreachable!(),
        }
    }

    fn line_break(&mut self) {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(_, dst_line) => *dst_line += 1,
            _ => unreachable!(),
        };
    }

    fn sourcemap(st: &mut Emit, ctx: &mut Context, target: &Uri) {
        let d = match ctx.proxy_state.get_doc(target) {
            Ok(doc) => doc,
            Err(_) => return,
        };
        let (source, path, tokens) = (&d.source, &d.path, d.tokens.iter());
        let src_id = st.add_source(source.as_str().into());

        match ctx.visited_sources.contains(&d.source_hash) {
            true => return,
            false => ctx.visited_sources.insert(d.source_hash),
        };

        let _ = Emit::sourcemap(st, ctx, ctx.defult_document);

        // DocumentDeclarationStatement
        st.line_break();
        st.add_token(0, 0, 0, src_id);
        st.line_break();

        let mut lt_ro_skip = false;
        let mut lt_ro = false;
        let mut lt_ro_offset = 0;
        let add_map =
            |dst_col: usize, pos: &LineCol, st: &mut Emit, lt_ro: bool, lt_ro_offset: usize| {
                let dst_col = match lt_ro {
                    true => dst_col + lt_ro_offset,
                    false => dst_col,
                };
                st.add_token(dst_col, pos.line, pos.col, src_id);
            };

        for t in tokens {
            match t {
                Token::Include(t) => add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::IncludePath(t) => {
                    // TODO: optimize
                    let (line, col) = (t.line_col.line, t.line_col.col);
                    let dep_lit = t.text.trim_matches(|c| ['\'', '"', '<', '>'].contains(&c));
                    let dep_path = ctx.proxy_state.path_resolver(&path, dep_lit);
                    let dep_uri = &Uri::from_file_path(dep_path.as_path()).unwrap();
                    let dep_link = ctx
                        .proxy_state
                        .get_doc(dep_uri)
                        .and_then(|d| Ok(d.link_stmt.clone()))
                        .unwrap_or_else(|_| {
                            let ref undefined_source = Source::new(dep_lit.into());
                            let undefined_ident = &DocumentIdentifier::new(undefined_source);
                            DocumentLinkStatement::new(undefined_source, undefined_ident).into()
                        });

                    // DocumentLinkStatement
                    st.line_break();
                    st.add_token(dep_link.left_offset, line, col, src_id);
                    st.add_token(dep_link.right_offset, 0, 0, !0);
                    st.line_break();

                    Emit::sourcemap(st, ctx, dep_uri);

                    // traling statements after include statement on the same line
                    st.line_break();
                }
                Token::RegionOpen(t) => {
                    add_map(0, &t.line_col, st, lt_ro, lt_ro_offset);
                    (lt_ro_skip = true, lt_ro_offset = t.len);
                }
                Token::LineTerminator(_) if lt_ro_skip => {
                    (lt_ro_skip = false, lt_ro = true);
                }
                Token::RegionClose(t) => add_map(0, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::LineTerminator(t) => {
                    add_map(t.col, &t, st, lt_ro, lt_ro_offset);
                    lt_ro = false;
                    st.line_break();
                }
                Token::CommonWithLineEnding(t) => {
                    add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset);
                    lt_ro = false;
                    st.line_break();
                }
                Token::Common(t) => add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::EOI(t) => add_map(t.col, &t, st, lt_ro, lt_ro_offset),
            }
        }
    }
}

/// Destination content
impl Emit {
    fn push_str(&mut self, str: &str) {
        match self {
            Emit::WithDstContent(dst_content) => dst_content.push_str(str),
            _ => unreachable!(),
        }
    }

    fn push(&mut self, char: char) {
        match self {
            Emit::WithDstContent(dst_content) => dst_content.push(char),
            _ => unreachable!(),
        }
    }

    fn content(st: &mut Emit, ctx: &mut Context, target: &Uri) {
        let d = match ctx.proxy_state.get_doc(target) {
            Ok(doc) => doc,
            Err(_) => return,
        };
        let (path, tokens, decl_stmt) = (&d.path, d.tokens.iter(), &d.decl_stmt);

        match ctx.visited_sources.contains(&d.source_hash) {
            true => return,
            false => ctx.visited_sources.insert(d.source_hash),
        };

        Emit::content(st, ctx, ctx.defult_document);
        st.push_str(decl_stmt);

        let mut lt_ro_skip = false;
        for t in tokens {
            match t {
                Token::Include(t) => (0..t.len).for_each(|_| st.push(' ')),
                Token::IncludePath(t) => {
                    let dep_lit = t.text.trim_matches(|c| ['\'', '"', '<', '>'].contains(&c));
                    let dep_path = ctx.proxy_state.path_resolver(&path, dep_lit);
                    let dep_uri = &Uri::from_file_path(dep_path.as_path()).unwrap();
                    let dep_link = ctx
                        .proxy_state
                        .get_doc(dep_uri)
                        .and_then(|d| Ok(d.link_stmt.clone()))
                        .unwrap_or_else(|_| {
                            let ref undefined_source = Source::new(dep_lit.into());
                            let undefined_ident = &DocumentIdentifier::new(undefined_source);
                            DocumentLinkStatement::new(undefined_source, undefined_ident).into()
                        });

                    st.push_str(&dep_link);
                    Emit::content(st, ctx, dep_uri);

                    // traling statements after include statement on the same line
                    st.push('\n');
                    (0..(t.line_col.col + t.len)).for_each(|_| st.push(' '));
                }
                Token::RegionOpen(t) => {
                    lt_ro_skip = true;
                    (0..(t.len - 1)).for_each(|_| st.push(' '));
                    st.push('`');
                }
                Token::LineTerminator(_) if lt_ro_skip => {
                    lt_ro_skip = false;
                }
                Token::RegionClose(t) => {
                    st.push('`');
                    st.push(';');
                    (0..(t.len - 2)).for_each(|_| st.push(' '));
                }
                Token::LineTerminator(_) => st.push('\n'),
                Token::CommonWithLineEnding(t) => st.push_str(t.text),
                Token::Common(t) => st.push_str(t.text),
                Token::EOI(_) => {}
            }
        }
    }
}
