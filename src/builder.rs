use std::collections::HashSet;
use std::sync::{Arc, OnceLock};

use async_lsp::lsp_types as lsp;
use async_lsp::lsp_types::Url as Uri;
use derive_more::Constructor;
use sourcemap::SourceMap;

use crate::parser::{LineCol, Token};
use crate::state::State;
use crate::types::{DocumentLinkStatement, Source, SourceHash, SourceMapBuilder};

pub const BUILD_FILE: &str = "build.js.emitted";

#[derive(Debug, Constructor)]
pub struct Build {
    pub content: String,
    pub uri: Uri,

    source_map: SourceMap,
    tokens_count: usize,
}

/// Forwarding
impl Build {
    pub fn sources(&self) -> HashSet<Source> {
        self.source_map
            .sources()
            .map(|s| Source::new(s.into()))
            .collect()
    }

    pub fn forward_src_position(
        &self,
        pos: &lsp::Position,
        pos_source: &Source,
    ) -> Option<lsp::Position> {
        let mut token: Option<sourcemap::Token> = None;

        if !self.sources().contains(pos_source) {
            return None;
        }

        for t in self.source_map.tokens() {
            if t.get_source() != Some(pos_source) {
                continue;
            }
            if t.get_src_line() == pos.line && t.get_src_col() <= pos.character {
                token = Some(t);
            }
            if t.get_src_line() > pos.line {
                break;
            }
        }

        if let Some(t) = token {
            let line = t.get_dst_line();
            let character = t.get_dst_col() + (pos.character - t.get_src_col());
            Some(lsp::Position::new(line, character))
        } else {
            None
        }
    }

    pub fn forward_src_range(
        &self,
        range: &lsp::Range,
        range_source: &Source,
    ) -> Option<lsp::Range> {
        let build_start_pos = self.forward_src_position(&range.start, range_source);
        let build_end_pos = self.forward_src_position(&range.end, range_source);
        match (build_start_pos, build_end_pos) {
            (Some(start), Some(end)) => Some(lsp::Range::new(start, end)),
            _ => None,
        }
    }

    pub fn forward_build_position(&self, pos: &lsp::Position) -> Option<(lsp::Position, Source)> {
        match self.source_map.lookup_token(pos.line, pos.character) {
            Some(t) if t.get_source().is_none() => None,
            Some(t) => {
                let line = t.get_src_line();
                let character = t.get_src_col() + (pos.character - t.get_dst_col());
                let source = t.get_source().expect("forward back token must have source");
                Some((
                    lsp::Position::new(line, character),
                    Source::new(source.into()),
                ))
            }
            _ => None,
        }
    }

    pub fn forward_build_range(&self, range: &lsp::Range) -> Option<(lsp::Range, Source)> {
        let source_start_pos = self.forward_build_position(&range.start);
        let source_end_pos = self.forward_build_position(&range.end);
        match (source_start_pos, source_end_pos) {
            (Some((start, source)), Some((end, _))) => Some((lsp::Range::new(start, end), source)),
            _ => None,
        }
    }
}

impl Build {
    #[tracing::instrument(skip_all, fields( doc = uri.as_str().split("/").last().unwrap() ))]
    pub fn create(state: &State, uri: &Uri, prev_build: Option<Arc<Self>>) -> Self {
        let (mut initial_buf, sources_cap, tokens_cap) = {
            match prev_build {
                Some(b) => (
                    String::with_capacity(b.content.len()),
                    b.sources().len(),
                    b.tokens_count,
                ),
                None => (String::new(), 0, 0),
            }
        };

        initial_buf.push_str("/** DO NOT EDIT THIS FILE. Generated build for '");
        initial_buf.push_str(uri.as_str().split("/").last().unwrap());
        initial_buf.push_str("' */\n");

        let default_doc = &state.get_default_doc();
        let new_ctx = || {
            let visited = HashSet::<SourceHash>::with_capacity(sources_cap);
            Context::new(state, default_doc, visited)
        };

        let (tokens_count_ref, source_map_ref) = (OnceLock::new(), OnceLock::new());
        let builder = SourceMapBuilder::with_capacity(tokens_cap, sources_cap);
        let mut emit_content_state = Emit::WithDstContent(initial_buf);
        let mut emit_sourcemap_state = Emit::WithSourceMapBuilderAndDstLine(builder, 1);
        let content_task = || Emit::content(&mut emit_content_state, &mut new_ctx(), uri);
        let sourcemap_task = || {
            Emit::sourcemap(&mut emit_sourcemap_state, &mut new_ctx(), uri);
            let (tokens_count, source_map) = match emit_sourcemap_state.finish(state) {
                EmitResult::TokensCountAndSourceMap(count, sm) => (count, sm),
                _ => unreachable!(),
            };
            tokens_count_ref.set(tokens_count).unwrap();
            source_map_ref.set(source_map).unwrap();
        };

        // TODO: rebuild only sourcemap on dep_hash eq prev
        rayon::join(sourcemap_task, content_task);

        let (tokens_count, source_map) = (
            tokens_count_ref.into_inner().unwrap(),
            source_map_ref.into_inner().unwrap(),
        );
        let content = match emit_content_state.finish(state) {
            EmitResult::Content(content) => content,
            _ => unreachable!(),
        };

        #[cfg(debug_assertions)]
        {
            use base64::prelude::{BASE64_STANDARD, Engine as _};

            let mut sm_json = Vec::new();
            let _ = source_map.to_writer(&mut sm_json);
            let sm_base64 = BASE64_STANDARD.encode(&sm_json);
            let build = format!(
                "{}\n//# sourceMappingURL=data:application/json;base64,{}",
                &content, sm_base64
            );
            let _ = std::fs::write(state.get_project().join(BUILD_FILE), build);
        }

        let emit_uri = (*state.get_doc(uri).unwrap().build_uri).clone();

        Self::new(content, emit_uri, source_map, tokens_count)
    }
}

#[derive(Constructor)]
struct Context<'a> {
    proxy_state: &'a State,
    defult_document: &'a Uri,
    visited_sources: HashSet<SourceHash>,
}

enum Emit {
    WithSourceMapBuilderAndDstLine(SourceMapBuilder, u32),
    WithDstContent(String),
}

enum EmitResult {
    TokensCountAndSourceMap(usize, sourcemap::SourceMap),
    Content(String),
}

impl Emit {
    fn finish(self, state: &State) -> EmitResult {
        match self {
            Emit::WithDstContent(dst_content) => EmitResult::Content(dst_content),
            Emit::WithSourceMapBuilderAndDstLine(b, _) => {
                EmitResult::TokensCountAndSourceMap(b.tokens.len(), b.into_sourcemap(state))
            }
        }
    }
}

/// SourceMap
impl Emit {
    fn add_source(&mut self, source: Arc<Source>) -> u32 {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(builder, _) => builder.add_source_with_id(source),
            _ => unreachable!(),
        }
    }

    fn add_token(&mut self, dst_col: u32, src_line: u32, src_col: u32, src_id: u32) {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(builder, dst_line) => {
                builder.tokens.push(sourcemap::RawToken {
                    dst_line: *dst_line,
                    dst_col,
                    src_line,
                    src_col,
                    src_id,
                    name_id: !0,
                    is_range: false,
                })
            }
            _ => unreachable!(),
        }
    }

    fn line_break(&mut self) {
        match self {
            Emit::WithSourceMapBuilderAndDstLine(_, dst_line) => *dst_line += 1,
            _ => unreachable!(),
        };
    }

    // #[tracing::instrument(skip_all)]
    // fn sm_time(st: &mut Emit, ctx: &mut Context, target: &Uri) {
    //     Emit::sourcemap(st, ctx, target);
    // }

    fn sourcemap(st: &mut Emit, ctx: &mut Context, target: &Uri) {
        let d = match ctx.proxy_state.get_doc(target) {
            Ok(doc) => doc,
            Err(_) => return,
        };
        let (source, path, tokens) = (&d.source, &d.path, d.tokens.iter());
        let src_id = st.add_source(source.clone());

        match ctx.visited_sources.contains(&d.source_hash) {
            true => return,
            false => ctx.visited_sources.insert(d.source_hash),
        };

        Emit::sourcemap(st, ctx, ctx.defult_document);

        // DocumentDeclarationStatement
        st.line_break();
        st.add_token(0, 0, 0, src_id);
        st.line_break();

        let mut lt_ro_skip = false;
        let mut lt_ro = false;
        let mut lt_ro_offset = 0u32;
        let add_map =
            |dst_col: u32, pos: &LineCol, st: &mut Emit, lt_ro: bool, lt_ro_offset: u32| {
                let dst_col = match lt_ro {
                    true => dst_col + lt_ro_offset,
                    false => dst_col,
                };
                st.add_token(dst_col, pos.line, pos.col, src_id);
            };

        for t in tokens {
            match t {
                Token::Include(t) => add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::IncludePath(t) => {
                    let dep_path = ctx.proxy_state.path_resolver(path, t.path);
                    let dep_uri = ctx.proxy_state.path_to_uri(&dep_path);
                    let (left_offset, right_offset, doc_uri) = if let Ok(uri) = dep_uri {
                        if let Ok(d) = ctx.proxy_state.get_doc(&uri) {
                            (d.link_stmt.left_offset, d.link_stmt.right_offset, Some(uri))
                        } else {
                            let stmt = DocumentLinkStatement::undefined();
                            (stmt.left_offset, stmt.right_offset, None)
                        }
                    } else {
                        let stmt = DocumentLinkStatement::undefined();
                        (stmt.left_offset, stmt.right_offset, None)
                    };

                    st.line_break();
                    st.add_token(left_offset, t.line_col.line, t.line_col.col, src_id);
                    st.add_token(right_offset, 0, 0, !0);
                    st.line_break();

                    if let Some(target) = doc_uri {
                        Emit::sourcemap(st, ctx, &target);
                    }

                    st.line_break(); // traling statements after include path on current line
                }
                Token::RegionOpen(t) => {
                    add_map(0, &t.line_col, st, lt_ro, lt_ro_offset);
                    lt_ro_skip = true;
                    lt_ro_offset = t.len;
                }
                Token::LineTerminator(_) if lt_ro_skip => {
                    lt_ro_skip = false;
                    lt_ro = true;
                }
                Token::RegionClose(t) => add_map(0, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::LineTerminator(t) => {
                    add_map(t.col, t, st, lt_ro, lt_ro_offset);
                    lt_ro = false;
                    st.line_break();
                }
                Token::CommonWithLineEnding(t) => {
                    add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset);
                    lt_ro = false;
                    st.line_break();
                }
                Token::Common(t) => add_map(t.line_col.col, &t.line_col, st, lt_ro, lt_ro_offset),
                Token::Eoi(t) => add_map(t.col, t, st, lt_ro, lt_ro_offset),
            }
        }
    }
}

/// Destination content
impl Emit {
    fn push_str(&mut self, str: &str) {
        match self {
            Emit::WithDstContent(dst_content) => dst_content.push_str(str),
            _ => unreachable!(),
        }
    }

    fn push(&mut self, char: char) {
        match self {
            Emit::WithDstContent(dst_content) => dst_content.push(char),
            _ => unreachable!(),
        }
    }

    // #[tracing::instrument(skip_all)]
    // fn content_time(st: &mut Emit, ctx: &mut Context, target: &Uri) {
    //     Emit::content(st, ctx, target);
    // }

    fn content(st: &mut Emit, ctx: &mut Context, target: &Uri) {
        let d = match ctx.proxy_state.get_doc(target) {
            Ok(doc) => doc,
            Err(_) => return,
        };
        let (path, tokens, decl_stmt) = (&d.path, d.tokens.iter(), &d.decl_stmt);

        match ctx.visited_sources.contains(&d.source_hash) {
            true => return,
            false => ctx.visited_sources.insert(d.source_hash),
        };

        Emit::content(st, ctx, ctx.defult_document);
        st.push_str(decl_stmt);

        let mut lt_ro_skip = false;
        for t in tokens {
            match t {
                Token::Include(t) => (0..t.len).for_each(|_| st.push(' ')),
                Token::IncludePath(t) => {
                    let dep_path = ctx.proxy_state.path_resolver(path, t.path);
                    if let Ok(dep_uri) = ctx.proxy_state.path_to_uri(&dep_path) {
                        if let Ok(dep_doc) = ctx.proxy_state.get_doc(&dep_uri) {
                            st.push_str(&dep_doc.link_stmt);
                            Emit::content(st, ctx, &dep_uri);
                        } else {
                            st.push_str(&DocumentLinkStatement::undefined())
                        };
                    } else {
                        st.push_str(&DocumentLinkStatement::undefined());
                    };

                    st.push('\n'); // traling statements after include path on current line
                    (0..(t.line_col.col + t.path.len() as u32 + 2)).for_each(|_| st.push(' '));
                }
                Token::RegionOpen(t) => {
                    lt_ro_skip = true;
                    (0..(t.len - 1)).for_each(|_| st.push(' '));
                    st.push('`');
                }
                Token::LineTerminator(_) if lt_ro_skip => {
                    lt_ro_skip = false;
                }
                Token::RegionClose(t) => {
                    st.push('`');
                    st.push(';');
                    (0..(t.len - 2)).for_each(|_| st.push(' '));
                }
                Token::LineTerminator(_) => st.push('\n'),
                Token::CommonWithLineEnding(t) => st.push_str(t.text),
                Token::Common(t) => st.push_str(t.text),
                Token::Eoi(_) => {}
            }
        }
    }
}
